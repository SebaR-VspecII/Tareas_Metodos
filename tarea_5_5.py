# -*- coding: utf-8 -*-
"""Tarea 5.5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sao0WGDiqkeMO3zbCUCzukGn6Mn5wb1b
"""

import numpy as np
import math

# Función que calcula la entropía de Shannon de una distribución Binomial (en bits)
def entropia_binomial(n, p):
    k = np.arange(n + 1)  # Valores posibles desde 0 hasta n

    # Logaritmo del coeficiente binomial usando gamma
    logC = np.array([math.lgamma(n + 1) - math.lgamma(i + 1) - math.lgamma(n - i + 1) for i in k])

    # Logaritmo de las probabilidades
    log_prob = logC + k * math.log(p) + (n - k) * math.log(1 - p)

    # Para evitar problemas numéricos, restamos el máximo
    max_log = np.max(log_prob)
    prob = np.exp(log_prob - max_log)
    prob = prob / prob.sum()  # Normalizamos

    # Solo consideramos probabilidades positivas
    mask = prob > 0
    H = -np.sum(prob[mask] * np.log2(prob[mask]))  # Entropía en bits

    return H

# -----------------------------
# Ejemplo de uso

p = 0.7       # Probabilidad de éxito
n = 2000      # Número de ensayos

H_exacta = entropia_binomial(n, p)  # Entropía exacta

# Aproximación normal (válida para n grande)
H_aprox = 0.5 * math.log2(2 * math.pi * math.e * n * p * (1 - p))

print(f"Entropía exacta para binomial (n={n}, p={p}) = {H_exacta:.6f} bits")
print(f"Entropía aproximada (normal) = {H_aprox:.6f} bits")

# Nota: 6.0
# Faltó justificar las fórmulas usadas y
# tomar el límite de n grande.
